{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b864a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125e2638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Continuous Bag of Words (CBOW) is one of the architectures used in the Word2Vec framework for learning word embeddings. CBOW is designed to predict a target word based on its context which consists of surrounding words within a fixed window size\".split()\n",
    "\n",
    "# Create vocabulary\n",
    "word_to_index = {word: i for i, word in enumerate(set(text))}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "VOCAB_SIZE = len(word_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38f0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "\n",
    "def one_hot_vector(word):\n",
    "    vec = np.zeros(VOCAB_SIZE)\n",
    "    vec[word_to_index[word]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6db57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(text, window_size=1):\n",
    "    X_train, Y_train = [], []\n",
    "    \n",
    "    for i in range(1, len(text) - 1):  # Ignore first and last word\n",
    "        context_left = one_hot_vector(text[i - 1])\n",
    "        context_right = one_hot_vector(text[i + 1])\n",
    "        target = one_hot_vector(text[i])\n",
    "        \n",
    "        # Concatenating both context words\n",
    "        X_train.append(np.concatenate((context_left, context_right)))\n",
    "        Y_train.append(target)\n",
    "    \n",
    "    return np.array(X_train), np.array(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e768d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39, 70), (39, 35))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate input and target data\n",
    "X_train, Y_train = generate_training_data(text)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba02a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "INPUT_SIZE = 2 * VOCAB_SIZE  # Twice the vocabulary size due to concatenation\n",
    "HIDDEN_SIZE = 10\n",
    "OUTPUT_SIZE = VOCAB_SIZE\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(HIDDEN_SIZE, INPUT_SIZE) * 0.01\n",
    "b1 = np.zeros((HIDDEN_SIZE, 1))\n",
    "W2 = np.random.randn(OUTPUT_SIZE, HIDDEN_SIZE) * 0.01\n",
    "b2 = np.zeros((OUTPUT_SIZE, 1))\n",
    "\n",
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z))\n",
    "    return exp_Z / exp_Z.sum(axis=0, keepdims=True)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(Z1, A1, Z2, A2, X, Y, alpha):\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - Y.T\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(Z1)\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aceb7c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 3.5554\n",
      "Embedding for 'of': [-0.00017233  0.0080626  -0.00184576  0.01039047 -0.01084843 -0.01407626\n",
      "  0.005025   -0.00321657 -0.00291391 -0.01705789]\n",
      "Epoch 100: Loss = 3.5077\n",
      "Embedding for 'of': [ 0.00128587  0.01443856 -0.00276269  0.01246338 -0.01130911 -0.01732401\n",
      "  0.00624983 -0.00244083 -0.00473443 -0.01705789]\n",
      "Epoch 200: Loss = 3.4821\n",
      "Embedding for 'of': [ 0.00354933  0.02296072 -0.00234266  0.01693458 -0.01446552 -0.0228438\n",
      "  0.00682977 -0.00173089 -0.00689776 -0.01693279]\n",
      "Epoch 300: Loss = 3.4673\n",
      "Embedding for 'of': [ 0.00732259  0.035925   -0.00139335  0.02509717 -0.01731113 -0.03012987\n",
      "  0.00690382 -0.00366802 -0.01170799 -0.01849587]\n",
      "Epoch 400: Loss = 3.4547\n",
      "Embedding for 'of': [ 0.01384386  0.05656166  0.0004662   0.03952402 -0.02107966 -0.04629145\n",
      "  0.00587515 -0.00856227 -0.01874481 -0.02511609]\n",
      "Epoch 500: Loss = 3.4362\n",
      "Embedding for 'of': [ 0.02494572  0.09018875  0.00385412  0.06473348 -0.02563999 -0.07250702\n",
      "  0.00232601 -0.01824084 -0.02983973 -0.03536054]\n",
      "Epoch 600: Loss = 3.3967\n",
      "Embedding for 'of': [ 0.04365178  0.14581152  0.00990798  0.1079413  -0.03073147 -0.11010222\n",
      " -0.00656992 -0.03622868 -0.04745174 -0.05171339]\n",
      "Epoch 700: Loss = 3.3023\n",
      "Embedding for 'of': [ 0.07502721  0.23726198  0.02054429  0.18090472 -0.03542543 -0.16475113\n",
      " -0.02551102 -0.0634898  -0.07485244 -0.07855316]\n",
      "Epoch 800: Loss = 3.0899\n",
      "Embedding for 'of': [ 0.12703704  0.38163023  0.03831703  0.3010913  -0.03723052 -0.23493839\n",
      " -0.05928433 -0.10200525 -0.12197295 -0.12472758]\n",
      "Epoch 900: Loss = 2.7237\n",
      "Embedding for 'of': [ 0.20982801  0.58352865  0.06520602  0.48658647 -0.03234674 -0.31107465\n",
      " -0.10162398 -0.15141543 -0.19423716 -0.19920685]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the model\n",
    "EPOCHS = 1000\n",
    "ALPHA = 0.1\n",
    "track_word = \"of\"  # Word to track during training\n",
    "track_index = word_to_index[track_word]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    Z1, A1, Z2, A2 = forward_propagation(X_train)\n",
    "    backward_propagation(Z1, A1, Z2, A2, X_train, Y_train, ALPHA)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        loss = -np.mean(np.sum(Y_train * np.log(A2.T + 1e-8), axis=1))\n",
    "        embedding_vector = W1[:, track_index]  # Extracting embedding of tracked word\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "        print(f\"Embedding for '{track_word}': {embedding_vector}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
